{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install langchain langchain-community transformers accelerate bitsandbytes sentencepiece torch -q","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-01T05:32:27.026805Z","iopub.execute_input":"2025-04-01T05:32:27.027205Z","iopub.status.idle":"2025-04-01T05:32:39.848710Z","shell.execute_reply.started":"2025-04-01T05:32:27.027181Z","shell.execute_reply":"2025-04-01T05:32:39.847891Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\nfrom langchain.llms import HuggingFacePipeline\nfrom langchain.agents import AgentType, initialize_agent\nfrom langchain.memory import ConversationBufferMemory\nfrom langchain.tools import Tool\nimport torch\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T05:32:39.850032Z","iopub.execute_input":"2025-04-01T05:32:39.850365Z","iopub.status.idle":"2025-04-01T05:33:03.736269Z","shell.execute_reply.started":"2025-04-01T05:32:39.850334Z","shell.execute_reply":"2025-04-01T05:33:03.735375Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip uninstall -y flash-attn -q\n!pip install ninja packaging -q\n!pip install flash-attn --no-build-isolation -q","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T05:33:03.737806Z","iopub.execute_input":"2025-04-01T05:33:03.738278Z","iopub.status.idle":"2025-04-01T05:33:28.406397Z","shell.execute_reply.started":"2025-04-01T05:33:03.738254Z","shell.execute_reply":"2025-04-01T05:33:28.405518Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM\n\nhf_token = \"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\"\nmodel_name = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_name, token=hf_token)\n\n# Load model with CPU offloading enabled\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=torch.float16,  # Efficient precision\n    device_map=\"auto\",  # Automatically allocates layers to GPU/CPU\n    load_in_4bit=True,  # Load model in 4-bit quantization\n    llm_int8_enable_fp32_cpu_offload=True,  # Offload large layers to CPU\n    token=hf_token\n)\n\nprint(\"✅ Model loaded with CPU offloading for limited GPU memory!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T05:33:28.408241Z","iopub.execute_input":"2025-04-01T05:33:28.408597Z","iopub.status.idle":"2025-04-01T05:35:30.871721Z","shell.execute_reply.started":"2025-04-01T05:33:28.408561Z","shell.execute_reply":"2025-04-01T05:35:30.871083Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"text_pipeline = pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    max_length=500,\n    temperature=0.7,\n    top_p=0.9,\n    max_new_tokens = 1024\n)\nllm = HuggingFacePipeline(pipeline=text_pipeline)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T05:35:30.872649Z","iopub.execute_input":"2025-04-01T05:35:30.872950Z","iopub.status.idle":"2025-04-01T05:35:30.882697Z","shell.execute_reply.started":"2025-04-01T05:35:30.872918Z","shell.execute_reply":"2025-04-01T05:35:30.881902Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def therapist_ask(input_text=\"\"):\n    \"\"\"Ensures therapist asks a question and waits for a patient response.\"\"\"\n    prompt = f\"\"\"\n    You are a therapist having a conversation with a patient.\n\n    - **Ask questions to assess the patient's mental health.**\n    - **Do NOT answer your own question.**\n    - Keep responses **short, natural, and empathetic**.\n    - Ensure each response **leads to another patient answer**.\n\n    Example conversation:\n    Patient: \"I feel exhausted all the time.\"\n    Therapist: \"That sounds really tough. Have you been sleeping okay?\"\n\n    Patient: {input_text}\n    Therapist:\n    \"\"\"\n\n    response = llm.invoke(prompt)\n    print(f\"\\n💡 DEBUG: Therapist response -> {response}\")  # ✅ Debugging line\n    return response.strip()\n\ntherapist_tool = Tool(\n    name=\"Therapist\",\n    func=therapist_ask,\n    description=\"Asks natural, empathetic questions to assess depression.\",\n    return_direct=True\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T05:35:30.883613Z","iopub.execute_input":"2025-04-01T05:35:30.883921Z","iopub.status.idle":"2025-04-01T05:35:30.897206Z","shell.execute_reply.started":"2025-04-01T05:35:30.883883Z","shell.execute_reply":"2025-04-01T05:35:30.896409Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def patient_reply(input_text=\"\"):\n    \"\"\"Ensures patient always responds after the therapist.\"\"\"\n    prompt = f\"\"\"\n    You are a patient speaking to a therapist about your mental health.\n    \n    - Respond naturally to the therapist's question.\n    - **Do NOT skip a response**—always say something.\n    - Keep responses **short but meaningful** (1-2 sentences).\n    - Show emotions where appropriate.\n    - Never stay silent.\n\n    Example conversation:\n    Therapist: \"How have you been feeling lately?\"\n    Patient: \"I’ve been feeling really down and unmotivated. Even simple tasks feel exhausting.\"\n\n    Therapist: {input_text}\n    Patient:\n    \"\"\"\n\n    response = llm.invoke(prompt)\n    print(f\"\\n💡 DEBUG: Patient response -> {response}\")  # ✅ Debugging line\n    return response.strip()\n\npatient_tool = Tool(\n    name=\"Patient\",\n    func=patient_reply,\n    description=\"Responds to therapist questions with emotions and details.\",\n    return_direct=True  # ✅ Ensures LangChain processes the response immediately\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T05:35:30.898033Z","iopub.execute_input":"2025-04-01T05:35:30.898324Z","iopub.status.idle":"2025-04-01T05:35:30.912634Z","shell.execute_reply.started":"2025-04-01T05:35:30.898296Z","shell.execute_reply":"2025-04-01T05:35:30.912063Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T05:35:30.913549Z","iopub.execute_input":"2025-04-01T05:35:30.913774Z","iopub.status.idle":"2025-04-01T05:35:30.933277Z","shell.execute_reply.started":"2025-04-01T05:35:30.913756Z","shell.execute_reply":"2025-04-01T05:35:30.932586Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from langchain.agents import AgentType\n\ntherapist_agent = initialize_agent(\n    tools=[therapist_tool],\n    llm=llm,\n    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,  # ✅ Forces a real conversation flow\n    verbose=True,\n    memory=memory,\n    handle_parsing_errors=True\n)\n\npatient_agent = initialize_agent(\n    tools=[patient_tool],\n    llm=llm,\n    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,  # ✅ Ensures patient participates\n    verbose=True,\n    memory=memory,\n    handle_parsing_errors=True\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T05:49:40.987001Z","iopub.execute_input":"2025-04-01T05:49:40.987321Z","iopub.status.idle":"2025-04-01T05:49:40.993227Z","shell.execute_reply.started":"2025-04-01T05:49:40.987296Z","shell.execute_reply":"2025-04-01T05:49:40.992414Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"num_rounds = 1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T05:35:30.953681Z","iopub.execute_input":"2025-04-01T05:35:30.953954Z","iopub.status.idle":"2025-04-01T05:35:30.966640Z","shell.execute_reply.started":"2025-04-01T05:35:30.953927Z","shell.execute_reply":"2025-04-01T05:35:30.966104Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"patient_input = \"I've been feeling really down lately.\"\n\nfor _ in range(5):  # Number of conversation turns\n    print(\"\\n🧑‍⚕️ Therapist is thinking...\")\n\n    # Therapist asks a question\n    therapist_response = therapist_agent.invoke({\"input\": patient_input})  # ✅ Pass input as dictionary\n    print(f\"\\n🧑‍⚕️ Therapist: {therapist_response}\")\n\n    print(\"\\n😞 Patient is responding...\")\n\n    # Patient responds to the therapist's question\n    patient_response = patient_agent.invoke({\"input\": therapist_response})  # ✅ Pass therapist's response\n    print(f\"\\n😞 Patient: {patient_response}\")\n\n    # Pass patient's response back to therapist\n    patient_input = patient_response\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T05:49:48.650974Z","iopub.execute_input":"2025-04-01T05:49:48.651307Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}