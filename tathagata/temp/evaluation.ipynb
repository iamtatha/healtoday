{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (4.50.2)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (0.29.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (2024.7.24)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (0.4.4)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests->transformers) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests->transformers) (2024.7.4)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76eefcfe594443a98d229fbb8923e966",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65c88bae490f49d18a363101f22a6dfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb5908b445f34527abc64600dd7efc96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fc838111be14598b0199c6b70a78375",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e68713d29863454a85a1d945fd030d5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f3897cec9fe4919a3d8a77e2bc075a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e153ff01b29b4adab092f6c199d156bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 162.47\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import math\n",
    "\n",
    "def calculate_perplexity(text, model_name=\"gpt2\"):\n",
    "    # Load tokenizer and model\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "    \n",
    "    # Tokenize input text and move to tensor\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    input_ids = inputs.input_ids\n",
    "    \n",
    "    # Disable gradient computation\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, labels=input_ids)\n",
    "        loss = outputs.loss  # Cross-entropy loss\n",
    "\n",
    "    # Compute perplexity\n",
    "    perplexity = math.exp(loss.item())\n",
    "    return perplexity\n",
    "\n",
    "# Example usage\n",
    "text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "ppl = calculate_perplexity(text)\n",
    "print(f\"Perplexity: {ppl:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Flesch Reading Ease': 88.23, 'Gunning Fog Index': 3.4}\n"
     ]
    }
   ],
   "source": [
    "import textstat\n",
    "\n",
    "def calculate_readability(text):\n",
    "    # Flesch-Kincaid Readability Score\n",
    "    flesch_score = textstat.flesch_reading_ease(text)\n",
    "    \n",
    "    # Gunning Fog Index\n",
    "    fog_index = textstat.gunning_fog(text)\n",
    "    \n",
    "    return {\n",
    "        \"Flesch Reading Ease\": flesch_score,\n",
    "        \"Gunning Fog Index\": fog_index\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "text = \"The quick brown fox jumps over the lazy dog. This sentence is simple and easy to read.\"\n",
    "readability_scores = calculate_readability(text)\n",
    "print(readability_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'entailment_score': 0.0002006716385949403, 'neutral_score': 0.0008906660368666053, 'contradiction_score': 0.9989086389541626, 'prediction': 'contradiction'}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "# Load RoBERTa trained on MNLI\n",
    "model_name = \"roberta-large-mnli\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# Function to check entailment\n",
    "def check_entailment(premise, hypothesis):\n",
    "    \"\"\"\n",
    "    Determines if the hypothesis (generated response) logically follows from the premise (previous conversation).\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(premise, hypothesis, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "\n",
    "    # Softmax to get probabilities\n",
    "    probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "    labels = [\"contradiction\", \"neutral\", \"entailment\"]\n",
    "\n",
    "    # Get highest probability label\n",
    "    prediction = labels[torch.argmax(probs).item()]\n",
    "    \n",
    "    return {\n",
    "        \"entailment_score\": probs[0][2].item(),\n",
    "        \"neutral_score\": probs[0][1].item(),\n",
    "        \"contradiction_score\": probs[0][0].item(),\n",
    "        \"prediction\": prediction\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "premise = \"It's so sunny outside\"\n",
    "hypothesis = \"The sky is totally full of clouds\"\n",
    "\n",
    "result = check_entailment(premise, hypothesis)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'allennlp_models'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mallennlp_models\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mallennlp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpredictors\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpredictor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Predictor\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Load the coreference resolution model\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'allennlp_models'"
     ]
    }
   ],
   "source": [
    "import allennlp_models\n",
    "from allennlp.predictors.predictor import Predictor\n",
    "\n",
    "# Load the coreference resolution model\n",
    "predictor = Predictor.from_path(\"https://storage.googleapis.com/allennlp-public-models/coref-spanbert-large-2021.03.10.tar.gz\")\n",
    "\n",
    "def resolve_coreferences(text):\n",
    "    \"\"\"\n",
    "    Resolves coreferences in the input text and returns a version with resolved entities.\n",
    "    \"\"\"\n",
    "    result = predictor.predict(document=text)\n",
    "\n",
    "    # Get clusters of coreferent mentions\n",
    "    clusters = result[\"clusters\"]\n",
    "    words = result[\"document\"]\n",
    "    \n",
    "    # Map coreferences to their main entity\n",
    "    coref_map = {}\n",
    "    for cluster in clusters:\n",
    "        main_entity = \" \".join(words[cluster[0][0] : cluster[0][1] + 1])  # First mention\n",
    "        for mention in cluster[1:]:  # Replace other mentions\n",
    "            coref_map[\" \".join(words[mention[0] : mention[1] + 1])] = main_entity\n",
    "\n",
    "    # Replace coreferent mentions in the text\n",
    "    resolved_text = []\n",
    "    for word in words:\n",
    "        resolved_text.append(coref_map.get(word, word))\n",
    "\n",
    "    return \" \".join(resolved_text)\n",
    "\n",
    "# Example usage\n",
    "text = \"Alice said she would go to the store. Later, he bought some apples.\"\n",
    "resolved_text = resolve_coreferences(text)\n",
    "print(resolved_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:559: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4136, 0.5864]])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 2 is out of bounds for dimension 0 with size 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 34\u001b[0m\n\u001b[1;32m     31\u001b[0m context \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI had an accident. I can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt walk.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     32\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOkay.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 34\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mmeasure_empathy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(result)\n",
      "Cell \u001b[0;32mIn[26], line 27\u001b[0m, in \u001b[0;36mmeasure_empathy\u001b[0;34m(context, response)\u001b[0m\n\u001b[1;32m     20\u001b[0m probs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39msoftmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(probs)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlow_empathy_score\u001b[39m\u001b[38;5;124m\"\u001b[39m: probs[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mitem(),\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmedium_empathy_score\u001b[39m\u001b[38;5;124m\"\u001b[39m: probs[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mitem(),\n\u001b[0;32m---> 27\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhigh_empathy_score\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mprobs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     28\u001b[0m }\n",
      "\u001b[0;31mIndexError\u001b[0m: index 2 is out of bounds for dimension 0 with size 2"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "# Load a DeBERTa model fine-tuned on EmpatheticDialogues\n",
    "model_name = \"microsoft/deberta-v3-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "def measure_empathy(context, response):\n",
    "    \"\"\"\n",
    "    Uses a fine-tuned model to assess how empathetic the response is.\n",
    "    Returns an empathy score.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(context, response, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "\n",
    "    # Softmax to get probabilities\n",
    "    probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "    \n",
    "    print(probs)\n",
    "\n",
    "    return {\n",
    "        \"low_empathy_score\": probs[0][0].item(),\n",
    "        \"medium_empathy_score\": probs[0][1].item(),\n",
    "        \"high_empathy_score\": probs[0][2].item()\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "context = \"I had an accident. I can't walk.\"\n",
    "response = \"Okay.\"\n",
    "\n",
    "result = measure_empathy(context, response)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "The model 'GPT2LMHeadModel' is not supported for text2text-generation. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'Qwen2AudioForConditionalGeneration', 'SeamlessM4TForTextToText', 'SeamlessM4Tv2ForTextToText', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'UMT5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sadness': 0.3279312551021576, 'joy': 0.0019324220484122634, 'love': 0.0007190885371528566, 'anger': 0.006600219290703535, 'fear': 0.659487783908844, 'surprise': 0.003329205559566617} {'sadness': 0.9626277685165405, 'joy': 0.01635679043829441, 'love': 0.0037591566797345877, 'anger': 0.00739735271781683, 'fear': 0.009084840305149555, 'surprise': 0.0007740959408693016}\n",
      "0.3217527861894074\n",
      "0.005991341200008826\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline, AutoModelForSequenceClassification, AutoTokenizer\n",
    "from typing import Dict, Tuple\n",
    "\n",
    "class EmpathicDialogueEvaluator:\n",
    "    def __init__(self):\n",
    "        self._load_models()\n",
    "        \n",
    "    def _load_models(self):\n",
    "        \"\"\"Load all required pretrained models\"\"\"\n",
    "        # Empathic Dialogue Model (using a fine-tuned T5 for dialogue)\n",
    "        self.edm = pipeline(\n",
    "            \"text2text-generation\",\n",
    "            model=\"microsoft/DialoGPT-medium\",\n",
    "            device=0 if torch.cuda.is_available() else -1\n",
    "        )\n",
    "        \n",
    "        # Emotion classification model (BERT-based)\n",
    "        self.emotion_model_name = \"bert-base-uncased\"\n",
    "        self.emotion_tokenizer = AutoTokenizer.from_pretrained(self.emotion_model_name)\n",
    "        self.emotion_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            \"bhadresh-savani/bert-base-uncased-emotion\",\n",
    "            num_labels=6\n",
    "        )\n",
    "        self.emotion_model.eval()\n",
    "    \n",
    "    def detect_emotion(self, text: str) -> Dict[str, float]:\n",
    "        \"\"\"Detect emotions in text\"\"\"\n",
    "        inputs = self.emotion_tokenizer(\n",
    "            text,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            padding=True\n",
    "        )\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.emotion_model(**inputs)\n",
    "        \n",
    "        logits = outputs.logits\n",
    "        probabilities = torch.softmax(logits, dim=1).squeeze()\n",
    "        \n",
    "        emotion_labels = ['sadness', 'joy', 'love', 'anger', 'fear', 'surprise']\n",
    "        emotion_scores = {label: float(prob) for label, prob in zip(emotion_labels, probabilities)}\n",
    "        \n",
    "        return emotion_scores\n",
    "    \n",
    "    def dot_product(self, dict1, dict2):\n",
    "        return sum(dict1[k] * dict2[k] for k in dict1 if k in dict2)\n",
    "    \n",
    "    def evaluate_empathy(self, context: str, response: str) -> Dict[str, float]:\n",
    "        \"\"\"Evaluate how empathic a response is to the given context\"\"\"\n",
    "        # Get context emotion\n",
    "        context_emotion = self.detect_emotion(context)\n",
    "        primary_context_emotion = max(context_emotion.items(), key=lambda x: x[1])[0]\n",
    "        \n",
    "        # Get response emotion\n",
    "        response_emotion = self.detect_emotion(response)\n",
    "        print(context_emotion, response_emotion)\n",
    "        \n",
    "        emotion_alignment = response_emotion.get(primary_context_emotion, 0.0)\n",
    "        # print(emotion_alignment)\n",
    "        print(self.dot_product(context_emotion, response_emotion))\n",
    "        print(context_emotion[primary_context_emotion] * response_emotion[primary_context_emotion])\n",
    "        \n",
    "        return {\n",
    "            # 'empathy_score': empathy_score,\n",
    "            'emotion_alignment': emotion_alignment,\n",
    "            'context_emotion': primary_context_emotion,\n",
    "            'response_emotion_distribution': response_emotion\n",
    "        }\n",
    "    \n",
    "    def full_pipeline(self, context: str, response: str) -> Tuple[str, Dict[str, float]]:\n",
    "        \"\"\"Complete pipeline from context to evaluated response\"\"\"\n",
    "        evaluation = self.evaluate_empathy(context, response)\n",
    "        return evaluation\n",
    "\n",
    "\n",
    "def empathy(context, response):\n",
    "    evaluator = EmpathicDialogueEvaluator()\n",
    "    \n",
    "    # Generate and evaluate response\n",
    "    evaluation = evaluator.full_pipeline(context, response)\n",
    "    \n",
    "    # print(f\"Context: {context}\")\n",
    "    # print(f\"Generated Response: {response}\")\n",
    "    # print(\"\\nEvaluation Metrics:\")\n",
    "    # # print(f\"Empathy Score: {evaluation['empathy_score']:.3f}\")\n",
    "    # print(f\"Emotion Alignment: {evaluation['emotion_alignment']:.3f}\")\n",
    "    # print(f\"Context Emotion: {evaluation['context_emotion']}\")\n",
    "    # print(\"Response Emotion Distribution:\")\n",
    "    # for emotion, score in evaluation['response_emotion_distribution'].items():\n",
    "    #     print(f\"  {emotion}: {score:.3f}\")\n",
    "    \n",
    "    \n",
    "context = \"I had an accident. I can't walk.\"\n",
    "response = \"I am so sorry to hear that. is there anything I can do to help you?\"\n",
    "empathy(context, response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "The model 'GPT2LMHeadModel' is not supported for text2text-generation. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'Qwen2AudioForConditionalGeneration', 'SeamlessM4TForTextToText', 'SeamlessM4Tv2ForTextToText', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'UMT5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sadness': 0.3279312551021576, 'joy': 0.0019324220484122634, 'love': 0.0007190885371528566, 'anger': 0.006600219290703535, 'fear': 0.659487783908844, 'surprise': 0.003329205559566617} {'sadness': 0.9626277685165405, 'joy': 0.01635679043829441, 'love': 0.0037591566797345877, 'anger': 0.00739735271781683, 'fear': 0.009084840305149555, 'surprise': 0.0007740959408693016}\n",
      "0.3217527861894074\n",
      "0.005991341200008826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "The model 'GPT2LMHeadModel' is not supported for text2text-generation. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'Qwen2AudioForConditionalGeneration', 'SeamlessM4TForTextToText', 'SeamlessM4Tv2ForTextToText', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'UMT5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sadness': 0.3279312551021576, 'joy': 0.0019324220484122634, 'love': 0.0007190885371528566, 'anger': 0.006600219290703535, 'fear': 0.659487783908844, 'surprise': 0.003329205559566617} {'sadness': 0.004103075247257948, 'joy': 0.9866077899932861, 'love': 0.002547743497416377, 'anger': 0.002644197316840291, 'fear': 0.002923656953498721, 'surprise': 0.0011735305888578296}\n",
      "0.005203376567147151\n",
      "0.0019281160451725537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "The model 'GPT2LMHeadModel' is not supported for text2text-generation. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'Qwen2AudioForConditionalGeneration', 'SeamlessM4TForTextToText', 'SeamlessM4Tv2ForTextToText', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'UMT5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sadness': 0.3279312551021576, 'joy': 0.0019324220484122634, 'love': 0.0007190885371528566, 'anger': 0.006600219290703535, 'fear': 0.659487783908844, 'surprise': 0.003329205559566617} {'sadness': 0.0009785917354747653, 'joy': 0.011624733917415142, 'love': 0.007527983281761408, 'anger': 0.00934939831495285, 'fear': 0.0016754867974668741, 'surprise': 0.9688438177108765}\n",
      "0.004740939373081144\n",
      "0.001104963075029955\n"
     ]
    }
   ],
   "source": [
    "context = \"I had an accident. I can't walk.\"\n",
    "response = \"I am so sorry to hear that. is there anything I can do to help you?\"\n",
    "empathy(context, response)\n",
    "\n",
    "context = \"I had an accident. I can't walk.\"\n",
    "response = \"Okay.\"\n",
    "empathy(context, response)\n",
    "\n",
    "context = \"I had an accident. I can't walk.\"\n",
    "response = \"Haha. So funny.\"\n",
    "empathy(context, response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "The model 'GPT2LMHeadModel' is not supported for text2text-generation. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'Qwen2AudioForConditionalGeneration', 'SeamlessM4TForTextToText', 'SeamlessM4Tv2ForTextToText', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'UMT5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sadness': 0.0006771081243641675, 'joy': 0.995418906211853, 'love': 0.0009584332001395524, 'anger': 0.0022520895581692457, 'fear': 0.0002604444161988795, 'surprise': 0.0004329569637775421} {'sadness': 0.00043299945537000895, 'joy': 0.9985278844833374, 'love': 0.000645835476461798, 'anger': 0.00012815202353522182, 'fear': 8.399751095566899e-05, 'surprise': 0.00018109528173226863}\n",
      "0.9939548356650308\n",
      "0.9939535345944392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "The model 'GPT2LMHeadModel' is not supported for text2text-generation. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'Qwen2AudioForConditionalGeneration', 'SeamlessM4TForTextToText', 'SeamlessM4Tv2ForTextToText', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'UMT5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sadness': 0.0006771081243641675, 'joy': 0.995418906211853, 'love': 0.0009584332001395524, 'anger': 0.0022520895581692457, 'fear': 0.0002604444161988795, 'surprise': 0.0004329569637775421} {'sadness': 0.005043392535299063, 'joy': 0.9863716959953308, 'love': 0.0017419852083548903, 'anger': 0.0034630002919584513, 'fear': 0.002347860485315323, 'surprise': 0.0010320197325199842}\n",
      "0.9818669765386012\n",
      "0.9818530347460026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "The model 'GPT2LMHeadModel' is not supported for text2text-generation. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'Qwen2AudioForConditionalGeneration', 'SeamlessM4TForTextToText', 'SeamlessM4Tv2ForTextToText', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'UMT5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sadness': 0.0006771081243641675, 'joy': 0.995418906211853, 'love': 0.0009584332001395524, 'anger': 0.0022520895581692457, 'fear': 0.0002604444161988795, 'surprise': 0.0004329569637775421} {'sadness': 0.0558466874063015, 'joy': 0.08871188014745712, 'love': 0.027992727234959602, 'anger': 0.8151504993438721, 'fear': 0.007115386892110109, 'surprise': 0.005182795226573944}\n",
      "0.09021001512726394\n",
      "0.08830548270437877\n"
     ]
    }
   ],
   "source": [
    "context = \"I just won the world championship brother!\"\n",
    "response = \"Wow! I am so happy for you. We should celebrate.\"\n",
    "empathy(context, response)\n",
    "\n",
    "context = \"I just won the world championship brother!\"\n",
    "response = \"Ok.\"\n",
    "empathy(context, response)\n",
    "\n",
    "context = \"I just won the world championship brother!\"\n",
    "response = \"Ugh! That sucks. Who cares man?\"\n",
    "empathy(context, response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt_tab: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1000)>\n"
     ]
    },
    {
     "ename": "MissingCorpusError",
     "evalue": "\nLooks like you are missing some required data for this feature.\n\nTo download the necessary data, simply run\n\n    python -m textblob.download_corpora\n\nor use the NLTK downloader to download the missing data: http://nltk.org/data.html\nIf this doesn't fix the problem, file an issue at https://github.com/sloria/TextBlob/issues.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/textblob/decorators.py:35\u001b[0m, in \u001b[0;36mrequires_nltk_corpus.<locals>.decorated\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 35\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/textblob/tokenizers.py:60\u001b[0m, in \u001b[0;36mSentenceTokenizer.tokenize\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return a list of sentences.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/nltk/tokenize/__init__.py:119\u001b[0m, in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;124;03mReturn a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;124;03musing NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m:param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 119\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43m_get_punkt_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/nltk/tokenize/__init__.py:105\u001b[0m, in \u001b[0;36m_get_punkt_tokenizer\u001b[0;34m(language)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03mA constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;124;03ma lru cache for performance.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;124;03m:type language: str\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/nltk/tokenize/punkt.py:1744\u001b[0m, in \u001b[0;36mPunktTokenizer.__init__\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1743\u001b[0m PunktSentenceTokenizer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m-> 1744\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/nltk/tokenize/punkt.py:1749\u001b[0m, in \u001b[0;36mPunktTokenizer.load_lang\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[0;32m-> 1749\u001b[0m lang_dir \u001b[38;5;241m=\u001b[39m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_params \u001b[38;5;241m=\u001b[39m load_punkt_params(lang_dir)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/nltk/data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/Users/tathagata.dey/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.12/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.12/share/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.12/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mMissingCorpusError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[74], line 31\u001b[0m\n\u001b[1;32m     28\u001b[0m user_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mm feeling really down today.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     29\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDon\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt worry, be happy! Everything is great!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mcheck_emotion_alignment\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[0;32mIn[74], line 12\u001b[0m, in \u001b[0;36mcheck_emotion_alignment\u001b[0;34m(user_input, response)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_emotion_alignment\u001b[39m(user_input, response):\n\u001b[1;32m     11\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compares the emotional intensity of user input and response.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m     user_emotions \u001b[38;5;241m=\u001b[39m \u001b[43mget_emotion_scores\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     response_emotions \u001b[38;5;241m=\u001b[39m get_emotion_scores(response)\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m# Example: Ensure sadness in input is not countered by excessive joy in response\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[74], line 7\u001b[0m, in \u001b[0;36mget_emotion_scores\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_emotion_scores\u001b[39m(text):\n\u001b[1;32m      6\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns emotion intensity scores from NRC Lexicon.\"\"\"\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m     emotion \u001b[38;5;241m=\u001b[39m \u001b[43mNRCLex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m emotion\u001b[38;5;241m.\u001b[39maffect_frequencies\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/nrclex.py:2873\u001b[0m, in \u001b[0;36mNRCLex.__init__\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m   2871\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext \u001b[38;5;241m=\u001b[39m text\n\u001b[1;32m   2872\u001b[0m blob \u001b[38;5;241m=\u001b[39m TextBlob(text)\n\u001b[0;32m-> 2873\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwords \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[43mblob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwords\u001b[49m)\n\u001b[1;32m   2874\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msentences \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(blob\u001b[38;5;241m.\u001b[39msentences)\n\u001b[1;32m   2875\u001b[0m build_word_affect(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/textblob/decorators.py:23\u001b[0m, in \u001b[0;36mcached_property.__get__\u001b[0;34m(self, obj, cls)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[0;32m---> 23\u001b[0m value \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m value\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/textblob/blob.py:625\u001b[0m, in \u001b[0;36mTextBlob.words\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    617\u001b[0m \u001b[38;5;129m@cached_property\u001b[39m\n\u001b[1;32m    618\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwords\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    619\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return a list of word tokens. This excludes punctuation characters.\u001b[39;00m\n\u001b[1;32m    620\u001b[0m \u001b[38;5;124;03m    If you want to include punctuation characters, access the ``tokens``\u001b[39;00m\n\u001b[1;32m    621\u001b[0m \u001b[38;5;124;03m    property.\u001b[39;00m\n\u001b[1;32m    622\u001b[0m \n\u001b[1;32m    623\u001b[0m \u001b[38;5;124;03m    :returns: A :class:`WordList <WordList>` of word tokens.\u001b[39;00m\n\u001b[1;32m    624\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 625\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m WordList(\u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_punc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/textblob/tokenizers.py:77\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, include_punc, *args, **kwargs)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, include_punc\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     70\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Convenience function for tokenizing text into words.\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \n\u001b[1;32m     72\u001b[0m \u001b[38;5;124;03m    NOTE: NLTK's word tokenizer expects sentences as input, so the text will be\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;124;03m    tokenized to sentences before being tokenized to words.\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m     75\u001b[0m     words \u001b[38;5;241m=\u001b[39m chain\u001b[38;5;241m.\u001b[39mfrom_iterable(\n\u001b[1;32m     76\u001b[0m         _word_tokenizer\u001b[38;5;241m.\u001b[39mitokenize(sentence, include_punc, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 77\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m     )\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m words\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/textblob/base.py:68\u001b[0m, in \u001b[0;36mBaseTokenizer.itokenize\u001b[0;34m(self, text, *args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mitokenize\u001b[39m(\u001b[38;5;28mself\u001b[39m, text, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     62\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return a generator that generates tokens \"on-demand\".\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \n\u001b[1;32m     64\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 0.6.0\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \n\u001b[1;32m     66\u001b[0m \u001b[38;5;124;03m    :rtype: generator\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 68\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (t \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/textblob/decorators.py:37\u001b[0m, in \u001b[0;36mrequires_nltk_corpus.<locals>.decorated\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[0;32m---> 37\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MissingCorpusError() \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merror\u001b[39;00m\n",
      "\u001b[0;31mMissingCorpusError\u001b[0m: \nLooks like you are missing some required data for this feature.\n\nTo download the necessary data, simply run\n\n    python -m textblob.download_corpora\n\nor use the NLTK downloader to download the missing data: http://nltk.org/data.html\nIf this doesn't fix the problem, file an issue at https://github.com/sloria/TextBlob/issues.\n"
     ]
    }
   ],
   "source": [
    "from nrclex import NRCLex\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "def get_emotion_scores(text):\n",
    "    \"\"\"Returns emotion intensity scores from NRC Lexicon.\"\"\"\n",
    "    emotion = NRCLex(text)\n",
    "    return emotion.affect_frequencies\n",
    "\n",
    "def check_emotion_alignment(user_input, response):\n",
    "    \"\"\"Compares the emotional intensity of user input and response.\"\"\"\n",
    "    user_emotions = get_emotion_scores(user_input)\n",
    "    response_emotions = get_emotion_scores(response)\n",
    "\n",
    "    # Example: Ensure sadness in input is not countered by excessive joy in response\n",
    "    if user_emotions['sadness'] > 0.3 and response_emotions['joy'] > 0.5:\n",
    "        aligned = False  # Response is overly positive\n",
    "    else:\n",
    "        aligned = True\n",
    "\n",
    "    return {\n",
    "        \"user_emotions\": user_emotions,\n",
    "        \"response_emotions\": response_emotions,\n",
    "        \"aligned\": aligned\n",
    "    }\n",
    "\n",
    "# Example\n",
    "user_input = \"I'm feeling really down today.\"\n",
    "response = \"Don't worry, be happy! Everything is great!\"\n",
    "\n",
    "print(check_emotion_alignment(user_input, response))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43d50e1d84d84145a654422b8a8e1764",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/836 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a258700bcf154eb58d91d88666cbf566",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fdda4b9ce4d475fbd86119e72d6c73c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/21.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ea28f63efcc4a1785e4016d9222450e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/435 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7a1ffe6374d44bcbc3da102b6d58180",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/619 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54f7bbfeabca4190957fc28560e0294b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ebc5ca2e1784097b54711d14e2c786c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e03dbe0d5da6453aa13dabe17787bc25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00002-of-00003.bin:   0%|          | 0.00/9.89G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af797b7bc64a4ae79bf564cb364f065e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00001-of-00003.bin:   0%|          | 0.00/9.88G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d72e97967a944ec4a736138c2411e420",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00003-of-00003.bin:   0%|          | 0.00/7.18G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Load Empathy Fine-Tuned Alpaca Model (if available on Hugging Face)\n",
    "model_name = \"tatsu-lab/alpaca-7b\"  # Replace with the fine-tuned version if available\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "def generate_response(prompt):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    output = model.generate(**inputs, max_length=150)\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# Example Input\n",
    "user_input = \"I'm feeling really lost and hopeless today.\"\n",
    "response = generate_response(user_input)\n",
    "print(\"Model Response:\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All Flax model weights were used when initializing BertForMaskedLM.\n",
      "\n",
      "Some weights of BertForMaskedLM were not initialized from the Flax model and are newly initialized: ['cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empathy Score: 0.99999964\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoModel, AutoModelForMaskedLM\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Load PsychBERT or MentalBERT (Fine-tuned on mental health data)\n",
    "model_name = \"mnaylor/psychbert-cased\"  # or \"yiyanghkust/bert-psychology\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_name, from_flax=True)\n",
    "\n",
    "def classify_empathy(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    outputs = model(**inputs)\n",
    "    scores = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "    return scores.detach().numpy()\n",
    "\n",
    "# Example Input\n",
    "response_text = \"I understand that you're feeling lost. I'm here to listen and support you.\"\n",
    "empathy_score = classify_empathy(response_text)\n",
    "final_empathy_score = np.max(empathy_score)\n",
    "print(\"Empathy Score:\", final_empathy_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (1,15,28996) and (1,15,28996) not aligned: 28996 (dim 2) != 15 (dim 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m empathy_score2 \u001b[38;5;241m=\u001b[39m classify_empathy(response_text)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Compute dot product\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m dot_product \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mempathy_score1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mempathy_score2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEmpathy Match Score:\u001b[39m\u001b[38;5;124m\"\u001b[39m, dot_product)\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (1,15,28996) and (1,15,28996) not aligned: 28996 (dim 2) != 15 (dim 1)"
     ]
    }
   ],
   "source": [
    "response_text1 = \"I had an accident today. probably I can't walk anymore\"\n",
    "empathy_score1 = classify_empathy(response_text)\n",
    "\n",
    "response_text2 = \"haha. That's so funny.\"\n",
    "empathy_score2 = classify_empathy(response_text)\n",
    "\n",
    "vec1 = empathy_score1.flatten() \n",
    "vec2 = empathy_score2.flatten()\n",
    "\n",
    "dot_product = np.dot(vec1, vec2)\n",
    "print(\"Empathy Match Score:\", dot_product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
