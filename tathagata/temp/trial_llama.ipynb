{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: I'm functioning within optimal parameters, thank you for asking! My processes are running smoothly, and I've been updated with the latest software patches. However, I'd love to hear from you - what's on your mind today?\n",
      "2: I have access to a vast collection of jokes, but let me try to come up with something that might tickle your funny bone! Here's one: A man walked into a library and asked the librarian, \"Do you have any books on Pavlov's dogs and SchrÃ¶dinger's cat?\" The librarian replied, \"It rings a bell, but I'm not sure if it's here or not.\" I hope that brought a smile to your face! Would you like another one?\n",
      "3: I don't have any information about our conversation prior to this point, as our interaction started with your request for a joke. However, I can tell you that the first message I received from you was \"Hey, how are you?\" If you'd like to recall what else we discussed before I shared the joke, I'm happy to chat with you again!\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.llms import Ollama\n",
    "from langchain.chains import ConversationChain\n",
    "# Initialize memory\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "# Load Ollama with Llama model\n",
    "llm = Ollama(model=\"llama3.2\")\n",
    "\n",
    "# Create conversation chain with memory\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=memory\n",
    ")\n",
    "\n",
    "# Example conversation\n",
    "print('1:', conversation.predict(input=\"Hey, how are you?\"))  \n",
    "print('2:', conversation.predict(input=\"Tell me a joke.\"))   \n",
    "print('3:', conversation.predict(input=\"What was my first question?\"))  # It remembers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration Done 1\n",
      "Iteration Done 2\n",
      "Iteration Done 3\n",
      "Iteration Done 4\n",
      "Iteration Done 5\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationChain\n",
    "import time\n",
    "\n",
    "therapist_model = \"llama3.2\"\n",
    "patient_model = \"llama3.2\"\n",
    "\n",
    "output_file = open(f\"results/{therapist}_{patient}_conversation.txt\", 'w')\n",
    "\n",
    "with open(\"prompts/therapist.txt\", \"r\") as file:\n",
    "    therapist_prompt = file.read()\n",
    "with open(\"prompts/patient.txt\", \"r\") as file:\n",
    "    patient_prompt = file.read()\n",
    "with open(\"prompts/therapist_end.txt\", \"r\") as file:\n",
    "    therapist_end_prompt = file.read()\n",
    "\n",
    "\n",
    "agent_1 = Ollama(model=therapist_model)\n",
    "agent_2 = Ollama(model=patient_model)\n",
    "\n",
    "memory_1 = ConversationBufferMemory()\n",
    "memory_2 = ConversationBufferMemory()\n",
    "\n",
    "conversation_1 = ConversationChain(llm=agent_1, memory=memory_1)\n",
    "conversation_2 = ConversationChain(llm=agent_2, memory=memory_2)\n",
    "\n",
    "message = therapist_prompt\n",
    "patient_initial = conversation_2.predict(input=patient_prompt)\n",
    "memory_2.save_context({\"input\": patient_prompt}, {\"output\": patient_initial})\n",
    "turns = 5\n",
    "\n",
    "for i in range(turns):\n",
    "    if i == turns-1:\n",
    "        response_1 = conversation_1.predict(input=f\"{therapist_end_prompt}{message}\")\n",
    "    else:\n",
    "        response_1 = conversation_1.predict(input=message)\n",
    "    memory_1.save_context({\"input\": message}, {\"output\": response_1})\n",
    "    # print(f\"Agent 1: {response_1}\")\n",
    "    output_file.write(f\"Therapist: \\n{response_1}\\n\\n\")\n",
    "    \n",
    "    response_2 = conversation_2.predict(input=response_1)\n",
    "    # response_2 = input('Agent 2:')\n",
    "    memory_2.save_context({\"input\": response_1}, {\"output\": response_2})\n",
    "    # print(f\"Agent 2: {response_2}\")\n",
    "    output_file.write(f\"Patient: \\n{response_2}\\n\\n\\n\")\n",
    "    \n",
    "    message = response_2  # Pass the response to the next agent\n",
    "    print('Iteration Done', i+1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dd926204df44f9094cc98c36002c8a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTScore F1: 0.1082\n"
     ]
    }
   ],
   "source": [
    "!pip install -q bert_score\n",
    "from bert_score import score\n",
    "\n",
    "# Example conversation context and response\n",
    "context = \"I'm feeling really tired after work today.\"\n",
    "response = \"You should take some rest and relax.\"\n",
    "\n",
    "# Compute BERTScore\n",
    "P, R, F1 = score([response], [context], lang=\"en\", model_type=\"bert-base-uncased\", rescale_with_baseline=True)\n",
    "\n",
    "# Print the F1 score (main metric)\n",
    "print(f\"BERTScore F1: {F1.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "float"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = float(f\"{F1.item():.4f}\")\n",
    "type(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {\n",
    "                \"entailment_score\": 0.23313108086585999,\n",
    "                \"neutral_score\": 0.7469388842582703,\n",
    "                \"contradiction_score\": 0.019930055364966393,\n",
    "                \"prediction\": \"neutral\"\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'entailment_score': 0.23313108086585999, 'neutral_score': 0.7469388842582703, 'contradiction_score': 0.019930055364966393, 'prediction': 'neutral'} <class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "print(d, type(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7469388842582703"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max({k: v for k, v in d.items() if isinstance(v, (int, float))}.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>total</th>\n",
       "      <th>Perplexity</th>\n",
       "      <th>Flesch Reading Ease</th>\n",
       "      <th>Gunning Fog Index</th>\n",
       "      <th>Entailemtn-Neutral-Consistency</th>\n",
       "      <th>Empathy</th>\n",
       "      <th>BERTScore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>llama3.2_llama3.2</td>\n",
       "      <td>6</td>\n",
       "      <td>10.503352</td>\n",
       "      <td>53.548333</td>\n",
       "      <td>7.64</td>\n",
       "      <td>0.798991</td>\n",
       "      <td>0.296869</td>\n",
       "      <td>0.38765</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Unnamed: 0  total  Perplexity  Flesch Reading Ease  \\\n",
       "0  llama3.2_llama3.2      6   10.503352            53.548333   \n",
       "\n",
       "   Gunning Fog Index  Entailemtn-Neutral-Consistency   Empathy  BERTScore  \n",
       "0               7.64                        0.798991  0.296869    0.38765  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel('../results/summary_metrics.xlsx')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Unnamed: 0': 0    llama3.2_llama3.2\n",
       " Name: Unnamed: 0, dtype: object,\n",
       " 'total': 0    6\n",
       " Name: total, dtype: int64,\n",
       " 'Perplexity': 0    10.503352\n",
       " Name: Perplexity, dtype: float64,\n",
       " 'Flesch Reading Ease': 0    53.548333\n",
       " Name: Flesch Reading Ease, dtype: float64,\n",
       " 'Gunning Fog Index': 0    7.64\n",
       " Name: Gunning Fog Index, dtype: float64,\n",
       " 'Entailemtn-Neutral-Consistency': 0    0.798991\n",
       " Name: Entailemtn-Neutral-Consistency, dtype: float64,\n",
       " 'Empathy': 0    0.296869\n",
       " Name: Empathy, dtype: float64,\n",
       " 'BERTScore': 0    0.38765\n",
       " Name: BERTScore, dtype: float64}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = dict(df)\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unnamed: 0\n",
      "total\n",
      "Perplexity\n",
      "Flesch Reading Ease\n",
      "Gunning Fog Index\n",
      "Entailemtn-Neutral-Consistency\n",
      "Empathy\n",
      "BERTScore\n"
     ]
    }
   ],
   "source": [
    "for key in d:\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'total': 4,\n",
       " 'Perplexity': 3.7585527613481076,\n",
       " 'Flesch Reading Ease': 18.395,\n",
       " 'Gunning Fog Index': 1.9,\n",
       " 'Entailment-Neutral-Consistency': 0.23554785549640656,\n",
       " 'Empathy': 0.16970778894631255,\n",
       " 'BERTScore': 0.139875}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = {\n",
    "    \"total\": 4,\n",
    "    \"Perplexity\": 3.7585527613481076,\n",
    "    \"Flesch Reading Ease\": 18.395,\n",
    "    \"Gunning Fog Index\": 1.9,\n",
    "    \"Entailment-Neutral-Consistency\": 0.23554785549640656,\n",
    "    \"Empathy\": 0.16970778894631255,\n",
    "    \"BERTScore\": 0.139875\n",
    "}\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BERTScore</th>\n",
       "      <td>0.139875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Empathy</th>\n",
       "      <td>0.169708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Entailment-Neutral-Consistency</th>\n",
       "      <td>0.235548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Flesch Reading Ease</th>\n",
       "      <td>18.395000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gunning Fog Index</th>\n",
       "      <td>1.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perplexity</th>\n",
       "      <td>3.758553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total</th>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        1\n",
       "BERTScore                        0.139875\n",
       "Empathy                          0.169708\n",
       "Entailment-Neutral-Consistency   0.235548\n",
       "Flesch Reading Ease             18.395000\n",
       "Gunning Fog Index                1.900000\n",
       "Perplexity                       3.758553\n",
       "total                            4.000000"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = {1: d}\n",
    "df = pd.DataFrame(d)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
